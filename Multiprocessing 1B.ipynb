{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Python's Multiprocessing Library, Part 1B\n",
    "\n",
    "Frank Neugebauer\n",
    "May 19, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import timeit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from multiprocessing import Pool\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1B. Tokenize Articles\n",
    "\n",
    "Created a function that tokenizes each of the articles from the previous step. Measured the time it takes to tokenize all of the articles for the different number of processes.\n",
    "\n",
    "A separate log file is part created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Processes\tTime to Process\n",
      "1\t\t1.7296\n",
      "2\t\t1.7208\n",
      "4\t\t1.7025\n",
      "8\t\t1.6892\n",
      "16\t\t1.6907\n"
     ]
    }
   ],
   "source": [
    "def start_logger():\n",
    "    logging.basicConfig(filename ='./log/log_b_%s.log' %\n",
    "                        datetime.strftime(datetime.now(), '%m%d%Y_%H%M%S'),\n",
    "                        level = logging.DEBUG, format='%(asctime)s %(message)s', datefmt='%m-%d%H:%M:%S')\n",
    "\n",
    "def read_json_directory_time(n_processes):\n",
    "    logging.debug('In read_json_directory_time')\n",
    "    TEST_CODE = 'articles = read_json_directory(' + str(n_processes) + ')'\n",
    "    SETUP_CODE = '''from __main__ import read_json_directory'''\n",
    "    time = timeit.timeit(TEST_CODE, setup=SETUP_CODE, number = 1)\n",
    "    return time\n",
    "\n",
    "def read_article_jsonl(file_paths):\n",
    "    articles = []\n",
    "    logging.debug('In read_article_json...')\n",
    "    for file_path in file_paths:\n",
    "        logging.debug('Reading the ' + file_path + ' file...')\n",
    "        wiki_file_full = pd.read_json(file_path, lines=True)\n",
    "        articles.append(wiki_file_full.to_dict())\n",
    "    return articles\n",
    "\n",
    "def read_json_directory(n_processes):\n",
    "    WIKI_DIR = '../../data/wikipedia//featured-articles'\n",
    "    logging.debug('In read_json_directory...')\n",
    "    logging.debug('Building paths...')\n",
    "    json_file_paths = [\n",
    "        entry.path\n",
    "        for entry in os.scandir(WIKI_DIR) if entry.name.endswith('.jsonl')\n",
    "    ]\n",
    "    logging.debug('Starting the pooling...')\n",
    "    articles = read_article_jsonl(json_file_paths)\n",
    "    logging.debug('There are ' + str(len(articles)) + ' dictionaries in the articles list.')\n",
    "    logging.debug('Finished building the article dictionary.')\n",
    "\n",
    "    with Pool(processes=n_processes) as pool:\n",
    "        articles = pool.map(read_article_jsonl, json_file_paths) \n",
    " \n",
    "    return articles\n",
    "\n",
    "def tokenize_time(n_processes):\n",
    "    logging.debug('In tokenize_time...')\n",
    "    SETUP_CODE = '''from __main__ import tokenize_documents, read_json_directory'''\n",
    "    TEST_CODE = 'articles = read_json_directory(' + str(n_processes) + ');' + \\\n",
    "                'tokenize_documents(articles,' + str(n_processes) + ')'\n",
    "    times = timeit.timeit(TEST_CODE, setup=SETUP_CODE, number = 1)\n",
    "    return times\n",
    "\n",
    "def tokenize_document(document):\n",
    "    logging.debug('Inside tokenize_document (singlular)...')\n",
    "    sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False,\n",
    "                                    sublinear_tf=True, tokenizer=final_clean)\n",
    "    tfidf_rep = sklearn_tfidf.fit_transform(document)\n",
    "    tfidf_array = tfidf_rep.toarray()\n",
    "    return tfidf_array\n",
    "\n",
    "def strip_extras(this_text):\n",
    "    logging.debug('Inside strip_extras...')\n",
    "    return_str = []\n",
    "    for text in this_text.values():\n",
    "        the_string = text[0].replace('\\n', '').replace('\\'', '')\n",
    "        return_str.append(the_string)\n",
    "\n",
    "    return return_str\n",
    "\n",
    "def final_clean(doc):\n",
    "    logging.debug('Inside final_clean...')\n",
    "    doc.lower().split(\" \")\n",
    "    return doc\n",
    "\n",
    "def tokenize_documents(all_documents, n_processes):\n",
    "    logging.debug('Inside tokenize_documents...')\n",
    "\n",
    "    df = pd.DataFrame(all_documents)\n",
    "\n",
    "    clean_string_list = []\n",
    "    texts = df['section_texts']\n",
    "    for text_dict in texts:\n",
    "        clean_string_list.append(strip_extras(text_dict))\n",
    "\n",
    "    text_strings = []\n",
    "    for this_string_array in clean_string_list:\n",
    "        text_strings.append(this_string_array[0])\n",
    "\n",
    "    final_docs = []\n",
    "    for doc in clean_string_list:\n",
    "        final_docs.append(final_clean(doc[0]))\n",
    "\n",
    "    tokens_np_array = tokenize_document(final_docs)\n",
    "    logging.debug('There are ' + str(tokens_np_array.size) + ' tokens in the article.')\n",
    "\n",
    "    with Pool(n_processes)as pool:\n",
    "        tokenized_documents = pool.map(self.tokenize_document, all_documents)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_logger()\n",
    "    logging.debug('Starting tokenization...')\n",
    "\n",
    "    times = []\n",
    "    n_proc = [1, 2, 4, 8, 16]\n",
    "    for this_proc in n_proc:\n",
    "        this_record = {}\n",
    "        time = tokenize_time(this_proc)\n",
    "        this_record['# Processes'] = this_proc\n",
    "        this_record['Time to Process'] = round(time, 4)\n",
    "        times.append(this_record)\n",
    "\n",
    "    print(\"# Processes\\tTime to Process\")\n",
    "    for i in times:\n",
    "        print(\"{}\\t\\t{}\".format(i['# Processes'],i['Time to Process']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
